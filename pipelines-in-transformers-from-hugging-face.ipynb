{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is created to learn and practice Transformers library from Hugging Face.\n\nFeel free to comment and share your ideas.\n\nPlease upvote if this notebook helped you. Thanks!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-07T23:06:47.363851Z","iopub.execute_input":"2023-03-07T23:06:47.364249Z","iopub.status.idle":"2023-03-07T23:06:59.075379Z","shell.execute_reply.started":"2023-03-07T23:06:47.364213Z","shell.execute_reply":"2023-03-07T23:06:59.073801Z"}}},{"cell_type":"code","source":"import transformers\n\n#Set to avoid warning messages.\ntransformers.logging.set_verbosity_error()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-built pipelines","metadata":{}},{"cell_type":"code","source":"# transformers library has various pre-biult pipelines for various NLP and CV tasks\nfrom transformers.pipelines import PIPELINE_REGISTRY\n\n#Get the list of tasks that are supported by Huggingface pipeline\n\npip_list = PIPELINE_REGISTRY.get_supported_tasks()\nfor i in pip_list: print(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get information about a specific task, pt - PyTorch, tf - TensorFlow\nprint(\"\\nDefault Model for Sentiment Analysis: \")\nprint(PIPELINE_REGISTRY.check_task('sentiment-analysis')[1].get('default'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nimport os\n\n#Load a pipeline. This will download the model checkpoint from huggingface and cache it \n#locally on disk. If model is already available in cache, it will simply use the cached version\n#Download will usually take a long time, depending on network bandwidth\n\nsentiment_classifier = pipeline(\"sentiment-analysis\")\n\n#Cache usually available at : <<user-home>>.cache\\huggingface\\hub\n\ncache_dir = os.path.expanduser('~') + \"/.cache/huggingface/hub\"\nprint(\"Huggingface Cache directory is : \", cache_dir)\n\n#Contents of cache directory\nos.listdir(cache_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predict sentiment using the pipeline\nsentiment_results=sentiment_classifier(\"This is a great course\")\nprint(sentiment_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#A second example\nsentiment_results=sentiment_classifier(\"The download speed is really bad\")\nprint(sentiment_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using a specific model in the pipeline\nsentiment_classifier = pipeline(task=\"sentiment-analysis\",\n                                model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n\nsentiment_result=sentiment_classifier(\"This is a great course\")\n\nprint(sentiment_result)\n\n#Contents of cache directory\nos.listdir(cache_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Named Entity Recognition","metadata":{}},{"cell_type":"code","source":"# extract NERs from text (person, company, location, date, custom, order_number, etc)\nfrom transformers import pipeline\n\ninput_text=\"Sam went to California on the 23rd of August. \\\nThere, he visited Google headquarters with John Smith and bought a cap for $23\"\n\nbasic_ner = pipeline(\"ner\")\n\nbasic_ner(input_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print model architecture\nprint(basic_ner.model)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print model configuration\nprint(basic_ner.model.config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using a Custom Model and tokenizer\nfrom transformers import AutoTokenizer, TFAutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/camembert-ner-with-dates\", \n                                          from_pt=True)\n\nmodel = TFAutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/camembert-ner-with-dates\",\n                                                          from_pt=True)\n\nprint(model.config.id2label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prediction\nenhanced_ner = pipeline('ner', \n                        model=model, \n                        tokenizer=tokenizer, \n                        aggregation_strategy=\"simple\")\nenhanced_ner(input_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Question answering","metadata":{}},{"cell_type":"code","source":"# importing answering pipeline from Transformers and pre-trained model deepset/minilm-uncased-squad2\n# model is trained on domain-specific limited text, fast and accurate\n\nfrom transformers import pipeline\n\n# providing context for the model\ncontext=\"\"\"\nEarth is the third planet from the Sun and the only astronomical object \nknown to harbor life. While large volumes of water can be found \nthroughout the Solar System, only Earth sustains liquid surface water. \nAbout 71% of Earth's surface is made up of the ocean, dwarfing \nEarth's polar ice, lakes, and rivers. The remaining 29% of Earth's \nsurface is land, consisting of continents and islands. \nEarth's surface layer is formed of several slowly moving tectonic plates, \ninteracting to produce mountain ranges, volcanoes, and earthquakes. \nEarth's liquid outer core generates the magnetic field that shapes Earth's \nmagnetosphere, deflecting destructive solar winds.\n\"\"\"\n\n# activating pipelone\nquan_pipeline = pipeline(\"question-answering\", \n                         model=\"deepset/minilm-uncased-squad2\")\n\n# generating answer for our question, based on our context\nanswer=quan_pipeline(question=\"How much of earth is land?\",\n             context=context)\nprint(answer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# low confidence answer, we can filter only high confidence answers for our task\nprint(\"\\nAnother question :\")\nprint(quan_pipeline( question=\"How are mountain ranges created?\",\n             context=context))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the SQuAD metric - to evaluate QA models (predicted vs correct answer) using multiple metrics\n\nfrom evaluate import load\nsquad_metric = load(\"squad_v2\")\n\n#Ignoring Context & Question as they are not needed for evaluation\n#This example is to showcase how the evaluation works based on match between the prediction\n#and the correct answer\n\ncorrect_answer=\"Paris\"\n\npredicted_answers=[\"Paris\",\n                 \"London\",\n                 \"Paris is one of the best cities in the world\"]\n\ncum_predictions=[]\ncum_references=[]\n\n# generate predicted answers score vs our correct answer\nfor i in range(len(predicted_answers)):\n    \n    #Use the input format for predictions\n    predictions = [{'prediction_text':predicted_answers[i], \n                    'id': str(i),\n                    'no_answer_probability': 0.}]\n    cum_predictions.append(predictions[0])\n    \n    #Use the input format for answers\n    references = [{'answers': {'answer_start': [1], \n                               'text': [correct_answer]}, \n                   'id': str(i)}]\n    cum_references.append(references[0])\n\n    # return the evaluation of our answers\n    results = squad_metric.compute(predictions=predictions,\n                                   references=references)\n    print(\"F1 is\", results.get('f1'), \n          \" for answer :\", predicted_answers[i])\n    \n#Compute for cumulative Results, count of answers, etc\ncum_results=squad_metric.compute(predictions=cum_predictions,\n                                 references=cum_references)\nprint(\"\\n Cumulative Results : \\n\",cum_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text summarization","metadata":{}},{"cell_type":"code","source":"verbose_text =\"\"\"\nEarth is the third planet from the Sun and the only astronomical object \nknown to harbor life. \nWhile large volumes of water can be found \nthroughout the Solar System, only Earth sustains liquid surface water. \nAbout 71% of Earth's surface is made up of the ocean, dwarfing \nEarth's polar ice, lakes, and rivers. \nThe remaining 29% of Earth's \nsurface is land, consisting of continents and islands. \nEarth's surface layer is formed of several slowly moving tectonic plates, \ninteracting to produce mountain ranges, volcanoes, and earthquakes. \nEarth's liquid outer core generates the magnetic field that shapes Earth's \nmagnetosphere, deflecting destructive solar winds.\n\"\"\"\n\nverbose_text = verbose_text.replace(\"\\n\",\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# selecting summarization pipeline\nfrom transformers import pipeline\n\n\nextractive_summarizer = pipeline(\"summarization\", \n                                 min_length=10, \n                                 max_length=100)\n\n#Extractive summarization\nextractive_summary=extractive_summarizer(verbose_text)\n\nprint(extractive_summary[0].get(\"summary_text\"))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Checkpoint used: \", extractive_summarizer.model.config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate with ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\nimport evaluate\n\nrouge_evaluator = evaluate.load(\"rouge\")\n\n#Evaluate exact match strings\nreference_text=[\"This is the same string\"]\npredict_text=[\"This is the same string\"]\n\neval_results=rouge_evaluator.compute(predictions=predict_text, \n                                     references=reference_text)\nprint(\"Results for Exact match\",eval_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate no-match strings\nreference_text=[\"This is the different string\"]\npredict_text=[\"Google can predict warm weather\"]\n\neval_results=rouge_evaluator.compute(predictions=predict_text, \n                                     references=reference_text)\nprint(\"\\nResults for no match\", eval_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate summary\neval_results=rouge_evaluator.compute(\n    predictions=[extractive_summary[0].get(\"summary_text\")], \n    references=[verbose_text])\n\nprint(\"\\nResults for Summary generated\", eval_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Natural Language Generation","metadata":{}},{"cell_type":"code","source":"# cintent creation\nfrom transformers import pipeline\n\ntext_generator = pipeline(\"text-generation\", \n                          model=\"gpt2\")\ntransformers.set_seed(1)\n\ninput_text=\"Natural Language Processing is a \\\ngrowing domain in machine learning\"\n\nsynthetic_text=text_generator(input_text,\n                              num_return_sequences=3,\n                              max_new_tokens=50)\n\nfor text in synthetic_text:\n    print(text.get(\"generated_text\") ,\"\\n-----------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bot conversation","metadata":{}},{"cell_type":"code","source":"# chatbot conversation example \nfrom transformers import  Conversation\n\nconversational_pipeline = pipeline(\"conversational\", \n                                   model=\"facebook/blenderbot_small-90M\")\n\nprint(conversational_pipeline.model.config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Sample inputs\nfirst_input=\"Do you have any hobbies?\"\nsecond_input = \"I like to watch movies\"\nthird_input = \"action movies\"\n\n#Create a context\nbot_conversation = Conversation(first_input)\n\nprint(\"\\nFirst Exchange: \\n--------------------\")\n\nconversational_pipeline(bot_conversation)\nprint(\" User Input:\", bot_conversation.past_user_inputs[0])\nprint(\" Bot Output:\", bot_conversation.generated_responses[0])\n\nprint(\"\\nSecond Exchange: \\n--------------------\")\nbot_conversation.add_user_input(second_input)\nconversational_pipeline(bot_conversation)\n\nprint(\" User Input:\", bot_conversation.past_user_inputs[1])\nprint(\" Bot Output:\", bot_conversation.generated_responses[1])\n\nprint(\"\\nThird Exchange: \\n--------------------\")\nbot_conversation.add_user_input(third_input)\nconversational_pipeline(bot_conversation)\n\nprint(\" User Input:\", bot_conversation.past_user_inputs[2])\nprint(\" Bot Output:\", bot_conversation.generated_responses[1])\n\nprint(\"\\nAccessing All Responses: \")\nprint(bot_conversation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Translation","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n\nsource_english=\"Acme is a technology company based in New York and Paris\"\n\ninputs_german = tokenizer(\n    \"translate English to German: \" + source_english,\n    return_tensors=\"pt\",\n)\noutputs_german = model.generate(\n    inputs_german[\"input_ids\"], \n    max_length=40)\n\nprint(\"German Translation: \",\n      tokenizer.decode(outputs_german[0], \n                       skip_special_tokens=True))\n\ninputs_french = tokenizer(\n    \"translate English to French: \" + source_english, \n    return_tensors=\"pt\",\n)\noutputs_french = model.generate(\n    inputs_french[\"input_ids\"], \n    max_length=40)\n\nprint(\"French Translation: \", \n      tokenizer.decode(outputs_french[0], \n                       skip_special_tokens=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Further steps","metadata":{}},{"cell_type":"markdown","source":"I this notebook I have demonstrated the various pre-built pipelines in Transformers library.\n\nThe nex step is to practice the Transformers deeper and to create an AI project using the transfer learning with Hugging Face.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}